\load[ctustyle3]
\worktype[B/EN]
\faculty{F8}
\department{Katedra Počítačových Systémů}
\title{NPU/AI Accelerator support for PikeOS real-time operating system}
\author{Michal Žáček}
\abstractEN{
	NPU (Neural Processing Unit) accelerators in PCs or notebooks are becoming more and more important these days.  The target platform considered in this thesis is the board TQMa8MPxL\urlnote{
https://www.tq-group.com/en/products/tq-embedded/arm-architecture/tqma8mpxl
} but results should apply to any device with the NPU accelerator running under Linux and/or PikeOS.
Neither the Linux kernel nor PikeOS (real-time embedded OS)
currently support AI/ML acceleration using NPUs.
The objective of the thesis is to conduct research of
open source frameworks with the aim to integrate a reasonably
good SW support for utilizing this accelerator to enhance the performance of
machine learning (ML) algorithms on these target platforms.

\begitems \style N
* Analyze and evaluate implementation of the SW support for utilizing NPU accelerators in representative open-source frameworks.
* Investigate modules or libraries used in these frameworks for communication with NPU accelerators.
* Review ML algorithms implemented in these frameworks and select most appropriate frameworks for testing.
* Design benchmark data sets for testing the ML performance of the selected frameworks on the given Linux platform.
* Test the performance of selected frameworks on the given Linux platform.
* Based on these findings, select the most appropriate framework for the integration into the Linux platform and elaborate an integration procedure based  on e.g. Yocto packaging system.
* Document problems encountered during the integration process.
* Propose a procedure for the integration of the SW support for the NPU accelerator on the PikeOS platform.
\enditems

}
\abstractCZ{D}
\declaration{D}
\date{2025}
\draft

\def\OpenVX{OpenVX™}
\def\VeriSilicon{VeriSilicon}
\def\NXP{NXP}
\def\textemdash{-}

\makefront

\chap Introduction

Our computing devices have come a long way in terms of speed
and of course in terms of operating complexity.
This complexity can hold back very general purpose processing units
in performance when it comes to highly specialized tasks.
For a long time GPUs were used where a large number
of specialized identical operations needed to be performed on
a vast array of differing data inputs,
as their specialized design for this use-case made them incomparably faster than
using a common general purpose CPU.
Though most often associated with their namesake of graphics,
they excel at many applications and have become a
valuable tool when working with neural networks.

Recently even more specialized hardware has become available
to meet the rising demand in having access to these technologies
in more portable devices or for use in embedded situations.
Neural Processing Units\fnote{Historically also called Versatile/Tensor/Intelligence Processing Units (VPU/TPU/IPU),
	however for simplicity's sake we will only use the term NPU and only deviate if a specific
	function or product uses one of the alternatives},
are becoming more common,
however these don't often come with universal drivers
and the most widespread standard today has no freely adoptable implementation.

\sec Goals

This thesis will initially lay out the defined and known terms used in the context of neural nets, such as hardware, alternate names of known concepts or file formats.
Following this we shall go through the existing solutions implementing both the communication with actually hardware in the form of modules followed by frontend APIs for NPU work.
We will then go through the advantages that this hardware should in theory provide followed by a practical test of the actual results on real hardware.
These will be packaged into demonstration packages than may be run to show off and/or verify these results.
Finally, we discuss what this means for PikeOS integration of NPU support.

\sec NPUs (Neural Processing Units)

Neural Processing Units (further shortened to NPU) are in general a type of hardware accelerator which is
optimized to efficiently handle Machine Learning workloads.
Our NPU in question is composed of a frontend that is used for
communication between the NPU and the rest of the board,
a parallel processing unit,
a neural network engine
along with a backing storage.
The device works with 4-element vectors of 16/32-bit IEEE floating-point or 8/16/32-bit integers either signed or unsigned.
The device implements the \OpenVX~hardware API
along with some extension and
\VeriSilicon~provides an official open-source library through which to call this API,
called the
Tensor Interface Module\urlnote{https://github.com/VeriSilicon/TIM-VX?tab=readme-ov-file}
(further shortened to TIM-VX) contains C++ bindings,
and Ovxlib contains C bindings\rfc{Does this get called by TIM-VX, does it use SDKs?}.
TIM-XV calls into one of two SDKs, the \VeriSilicon~Unified \OpenVX~SDK\rfc{TODO}
supporting both compiling and running using either the GPU and NPU units,
and the\rfc{proprietary?} VIP-Lite SDK that only contains the runtime for the NPU.
TIM-VX is used as the target for third-party frameworks targetting the board's NPU.

The device also contains internal facilities for profiling purposes.\rfc{TODO}

\rfc{Include image from https://raw.githubusercontent.com/VeriSilicon/TIM-VX/8494275d7608942aa584c9c13bd5e2d77be9906c/docs/image/timvx_overview.svg}

\chap File Formats

\sec Model File Properties
\secc Model conversion
eIQ toolkit
In the converted model,
the neutronGraph node is already generated. The neutron-delegate only captures the neutronGraph node and
offloads the work to Neutron-S. Inline compilation is not supported yet. [page 6]

Apparently there are ways to compile a model for a specific NPU,
even multiple models so they efficiently live together
in memory.
\urlnote{https://coral.ai/docs/edgetpu/compiler/}

TensorFlow is used in preparation and then compiled to a lighter LiteRT format.
\urlnote{https://ai.google.dev/edge/litert/models/convert}

\secc Quantization
\rfc{a.k.a. using narrower floats than the model was built with}
Since the NPU itself is built for
8/16-bit wide floats \cite[IMxMachineLeLfRe2024].

\sec Keras
\sec Safetensors

\midinsert \clabel[safetensordef]{SafeTensor format structure}
\ctable{ll}{
	Size & Content \crl
	8 bytes & size of header \cr
	N bytes & header \cr
	rest & byte-buffer (little-endian)
}
\caption/t SafeTensor format structure
\endinsert

Header\urlnote{https://github.com/huggingface/safetensors}:
JSON UTF-8 string, each top-level key is a tensor name the value of which contains a dictionary specifying the dtype, shape and data_offsets.

A schema\urlnote{https://github.com/huggingface/safetensors/blob/main/docs/safetensors.schema.json} is available.
It allows partial loading

\sec Pickle

	{\bf\Red This is dangerous to use.}
Since pickling allows arbitrary instances of classes,
malicious code may be inserted as well into any model downloaded.

An example taken from \cite[ExploitingPythHamann2020].
\verbinput \hisyntax{python} (-) pickle_exploit.py

\sec Block map \code{.bmap}

Relates to Yocto project's \code{bmaptool}\urlnote{https://github.com/yoctoproject/bmaptool}.
\rfc{It's not yocto but forked from intel?}

\sec System Package Data Exchange \code{.spdx}

A Bill of Materials of all the included
packages used to build an image including all the versions
and metadata.
\urlnote{https://en.wikipedia.org/wiki/Software_Package_Data_Exchange}

\sec OpenEmbedded Image Creator \code{.wic}

Should your device require
multiple partitions.\urlnote{https://docs.yoctoproject.org/dev/dev-manual/wic.html}
Use if \code{bmap} isn't supported as it doesn't support
sparsness, etc.\urlnote{citation needed}

\secc OpenEmbedded kickstart file \code{.wks}

Contains build commands for the \code{wic} command.
\urlnote{https://docs.yoctoproject.org/dev/dev-manual/wic.html}

\secc Flattened Device Tree \code{.fdt} / Device Tree Source \code{.dts???}

\code{.dts} describes hardware.\urlnote{https://wiki.freebsd.org/FlattenedDeviceTree\#Device\_tree\_source\_.28DTS.29}
Afterwards compiled into \code{.dtb}.

\chap Setting up the environment

Our testing environment consists of a Yocto distribution,
running on our afformentioned chip.


\begtt
Build Configuration:
BB_VERSION           = "2.8.0"
BUILD_SYS            = "x86_64-linux"
NATIVELSBSTRING      = "universal"
TARGET_SYS           = "aarch64-poky-linux"
MACHINE              = "tqma8mpxl-mba8mpxl"
DISTRO               = "fsl-imx-wayland"
DISTRO_VERSION       = "6.6-scarthgap"
TUNE_FEATURES        = "aarch64 armv8a crc crypto"
TARGET_FPU           = ""
meta
meta-poky            = "HEAD:200d12b6a58ad961d60a7774ca0f7a9d29498724"
meta-oe
meta-python
meta-multimedia      = "HEAD:72018ca1b1a471226917e8246e8bbf9a374ccf97"
meta-freescale       = "HEAD:0627128b341cfb2bef7a0832ce8cac0ce1127f13"
meta-qt6             = "HEAD:586a6cb5aec755803a3be3cec359baafe89d6432"
meta-tq              = "HEAD:257b8c0b4b6df3bb27fb69bd2312dd254c73fed3"
meta-imx-ml
meta-imx-sdk
meta-imx-bsp         = "HEAD:219f6d04a4c339eb6f2dc626f944bbdf9a716ff5"
meta-arm
meta-arm-toolchain   = "HEAD:950a4afce46a359def2958bd9ae33fc08ff9bb0d"
meta-freescale-distro = "HEAD:b9d6a5d9931922558046d230c1f5f4ef6ee72345"
meta-overlay         = "<unknown>:<unknown>"
meta-virtualization  = "HEAD:6f3c1d8f90947408a6587be222fec575a1ca5195"
meta-filesystems
meta-networking      = "HEAD:72018ca1b1a471226917e8246e8bbf9a374ccf97"
meta-tpm
meta-parsec          = "HEAD:459d837338ca230254baa2994f870bf6eb9d0139"
meta-clang           = "HEAD:2b7433611d80f6d0ee1b04156fa91fc73d3c2665"
\endtt

\code{meta-overlay} here is our custom package for the purposes
of thi thesis.

We use both the BSP and related layers directly from the TQ website,
followed by the \NXP{} meta-imx layer,
cloned via the \code{repo} utility from their manifest
repository \urlnote{https://github.com/nxp-imx/imx-manifest.git}.

\begtt
repo init \
  -u https://github.com/nxp-imx/imx-manifest.git \
  -b imx-linux-scarthgap \
  -m imx-6.6.52-2.2.0.xml
\endtt

Following this we configure a build directory
with the Machine: \code{tqma8mpxl-mba8mpx}
and Distro: \code{fsl-imx-wayland}.

Flashing is then done with the command:
\code{sudo uuu -v -b emmc_all ./images/imx-boot-tqma8mpxl-mba8mpxl-mfgtool.bin-flash_spl_uboot ./images/imx-image-full-tqma8mpxl-mba8mpxl.rootfs.wic}

\sec Kernel Tests

The VSOCK Makefile has no install goal, yet the recipe tries to call it,
so we just disable that part of the recipe with the following:

\begtt
PACKAGECONFIG:kernel-tools = ""
\endtt

\sec ONNXRuntime

We change the original ONNXRuntime recipe's
source from NXP to the official repo
as it now supports our NPU through the use of the
TIM-VX library:

\begtt
ONNXRUNTIME_SRC ?= "gitsm://github.com/microsoft/onnxruntime.git"
SRC_URI = "${ONNXRUNTIME_SRC};nobranch=1;protocol=https
# Rel-1.21.0
SRCREV = "e0b66cad282043d4377cea5269083f17771b6dfc"
\endtt

ONNXRuntime's configuration script can't by default find our
installation of TIM-VX, so we must assist it a bit,
in addition to adding it to \code{(R)DEPENDS}:

\begtt
DEPENDS = "libpng zlib tim-vx tvm"
RDEPENDS:${PN} = "tim-vx tvm"

do_configure:prepend () {
    export TIM_VX_INSTALL="/usr"
}
\endtt

Next we must also actually enable the use of this library:

\begtt
EXTRA_OECMAKE += "\
    -Donnxruntime_USE_VSINPU=ON \
    -Donnxruntime_USE_TVM=ON \
"
\endtt

As of writing the main \code{1.21.0} version of ONNXRuntime
is the first to support VSINPU, however
the release version had broken support for
targets with no fp16 support,
therefore a pr's commit with the fix needs to be used.
\code{GitHub - Issue: 23957, PR: 23978}
It is very fast on track to be merged into main,
but now we apply these few commits with a patch.

\begtt
SRC_URI:append = " file://fajin-corp_gh_pr_23978.patch"
\endtt

Another patch is required though as the function
\code{VSINPUExecutionprovider::GetCapability}
in the file \code{vsinpu_execution_provider.cc}
calls a logger, yet omits to declare it so
we must add it manually.

\begtt
SRC_URI:append = " file://fix_logger.patch"
\endtt

The call to \code{save_build_and_ppackage_info}
in \code{setup.py},
causes an error.
It is only a buildinfo log,
so simply removing it only creates a warning
when importing the module without hindering further use.

\sec TIM-VX

This was simply needed to be updated to version 1.2.22.
\rfc{Specify the role of TIM-VX}

\sec OpenCV

The build recipe for OpenCV is patchable,
we add TIM-VX to dependencies,
enable it in OpenCV and again
point the configuration script at our installation
directory.

\begtt
EXTRA_OECMAKE:append = "\
    -D BUILD_opencv_gapi=OFF \
    -D WITH_TIMVX=ON \
    -D TIMVX_INSTALL_DIR=/usr/lib \
"

DEPENDS:append = " tim-vx"
RDEPENDS:${PN}:append = " tim-vx"
\endtt

\sec TVM

TVM is currently not yet working
as it builds but requires
\code{python3-scipy} at runtime,
when trying to run inference,
which I haven't managed to install properly yet.

\chap The Graph Workflow

\sec Python Subclasses modelling Tensor functions

Most frameworks analyzed use Python,
and all seem to use a similar form of abstraction.
They declare some class, in the case of PyTorch
this would be \code{torch.nn.Module}\cite[ModulePytorc],
and a Model is defined by a class that inherits
from this root,
and overrides one or more specific methods.

For efficiency's and compatibility's sake these functions may
later be compiled into other forms
especially when serialized into the form of any model file.

In the case of ONNX specifically we may take
the example code of:

\begtt \hisyntax{python}
class OnnxModule(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x + 3
\endtt

Which may be seen by the ONNX exporter as:

\begtt
class GraphModule(torch.nn.Module):
    def forward(self, x: "f32[100, 128]"):
         # File: /home/michal_atlas/cl/thesis/py/test_rig/prepare/onnx.py:48 in forward, code: return x + 3
        scalar_tensor_default: "f32[]" = torch.ops.aten.scalar_tensor.default(3, dtype = torch.float32)
        add: "f32[100, 128]" = torch.ops.aten.add.Tensor(x, scalar_tensor_default);  x = scalar_tensor_default = None
        return (add,)
\endtt

Before being compiled into low-level representation like so:

\begtt
graph(
    name=main_graph,
    inputs=(
        %"x"<FLOAT,[100,128]>
    ),
    outputs=(
        %"add"<FLOAT,[100,128]>
    ),
) {
    0 |  # node_Constant_0
         %"val_0"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(3), name=None)}
    1 |  # node_Cast_1
         %"scalar_tensor_default"<FLOAT,[]> ⬅️ ::Cast(%"val_0") {to=FLOAT}
    2 |  # node_Add_2
         %"add"<FLOAT,[100,128]> ⬅️ ::Add(%"x", %"scalar_tensor_default")
    return %"add"<FLOAT,[100,128]>
}
\endtt

These models also always have a static type of input and outputs
that may be unconstrained by the Python class which implements it
but must be specified explicitly when exporting.
The part of the type that determines the size of a tensor
is usually called their "shape" in this context.
We set our input shape to \code{(100,128)} in this example
so the exported graph has annotations like \code{f32[100, 128]}.

The subset of python supported is quite large,
even code such as \code{[ sum(n) for n in x ]}
will successfully compile into a graph,
the efficiency of which depends heavily on
the type of code,
for example the above generates a massive
array of round-robin addition nodes.
Of course calling external functions is also supported
and these are inlined and compiled as if written
directly inside forward.

If one wishes to have control over what
exactly their code compiles to,
it is better to directly use modules from the given library.

These can be initialized under \code{__init__}
as instance variables and then utilized in
the \code{forward} function like so:

\begtt \hisyntax{python}
def __init__(self):
  super(..., self).__init__()
  self.relu = nn.ReLU()

def forward(self, x):
  x = self.relu(x)
  return x
\endtt

It is also useful to note that this code defines\rfc{where??}
string names for the inputs and outputs of a graph,
these are then useful for retreiving outputs or setting inputs.

.\rfc{List of transpilers perhaps}
\begtt
[torch.onnx] Obtain model graph for `OnnxModule()` with Torch Script... ❌
[torch.onnx] Obtain model graph for `OnnxModule()` with internal Dynamo apis...
\endtt

\secc \code{__init__}

\secc \code{forward} (mandatory)

This is the crux of the pipeline,
it defines the actual operations performed on
the inputs and returns what the Graph node would have
as outputs.
\rfc{How are many inputs/outputs handled?}

\secc Movement functions

Methods such as \code{.to()}/\code{.cpu()}/\code{.ipu()},
cause our model to be run on the given target device.
However, this also hints to the given framework
what the target is which is also taken into account
in cases of optimization.

\sec Compiling down to hardware
\rfc{PyTorch at least seems to use something called Triton}

It seems that these frameworks really do
compile large parts of the python language
into one of many target IRs.

\begtt\hisyntax{python}
def foo(x):
  a = [2] + [ 3 for _ in range(2) ]
  return x * a[0] + 1
\endtt

\begtt
print(torch.jit.script(foo).code)

def foo(x: Tensor) -> Tensor:
  _0 = [2]
  _1 = annotate(List[int], [])
  for _2 in range(2):
    _3 = torch.append(_1, 3)
  a = torch.add(_0, _1)
  return torch.add(torch.mul(x, a[0]), 1)
\endtt

\begtt
print(torch.fx.symbolic_trace(foo).graph)

graph():
  %x : [num_users=1] = placeholder[target=x]
  %mul : [num_users=1] = call_function[target=operator.mul](args = (%x, 2), kwargs = {})
  %add : [num_users=1] = call_function[target=operator.add](args = (%mul, 1), kwargs = {})
  return add
\endtt

\begtt
print(torch.export.export(Foo(), (2,)).graph)

graph():
  %x : [num_users=0] = placeholder[target=x]
  return (5,)
\endtt
\rfc{Reference Manual 13.16 for NPU}

\chap Hardware Specifics

\sec NPU Contents

\cite[IMx8mPlusAp]

\secc Parallel Processing Unit (PPU)

SIMD4, with 4 units with 256 threads each.

\secc Neural Network Engine

Does 1152 MAC operations per clock cycle.

\glos {MAC}{Multiply–accumulate operation $a \leftarrow a + (b \times c)$}

\secc Tensor Processor

Supported operations are

\ctable{cl}{
Pooling & Max, average \cr
Unpooling & Yes \cr
Activation & ReLU, Leaky ReLU (LUT for other types) \cr
Normalization & Yes \cr
Region Proposal Support & Yes \cr
}

\secc OpenVX Hardware Support?


\sec Configuration Environment Variables read by TIM-VX
.\rfc{cite Machine Learning Guide[6.1.2]}

\begitems \style d
* {\code{USE_GPU_INFERENCE}} As the NPU and GPU share this driver,
TIM-VX uses the value of this variable to determine
which to use, "1" for GPU or "0" for NPU.
\enditems

\secc Profiling
\begitems \style d
* {\code{CNN_PERF}} Prints how long operations take. Requires \code{VIV_VX_DEBUG_LEVEL=1} and implied by \code{VIV_VX_PROFILE}.
* {\code{NN_EXT_SHOW_PERF}} Shows the details of how the compiler determines performance.
* {\code{VIV_VX_PROFILE}} Enables creation of \code{vprofiler_xxx.vpd} files which may be examined using the Vivante vAnalyzer tool from the Vivante VDK.
Information is either per-node (value: \code{"1"}) or per-graph (value: \code{"2"}).
* {\code{VIV_VX_DEBUG_LEVEL}} Prints extra debug information.
* {\code{VIV_MEMORY_PROFILE}} Only applies to CPU/GPU.
\enditems

\secc Model Caching
\begitems \style d
* {\code{VIV_VX_ENABLE_CACHE_GRAPH_BINARY}} Enables saving of the compiled graph to disk with a hash so that the next time the same graph would be compiled, this \code{*.nb} file will be loaded instead. In addition to this the warmup time may take more than one inference\rfc{why?}.
* {\code{VIV_VX_CACHE_BINARY_GRAPH_DIR}} Directory to save the cache files to.
\enditems

\sec Power Modes

The NPU can be set to 4 different states,
\begitems \style d
* {On} Standard full-power
* {Off} Can be powered off, since after leaving this state the device is reinitialized
* {Idle} Clock speed lowered to $1/64^{th}$
* {Suspend} Idle clock speed and requires some time to reach idle
\enditems
\rfc{Performance Counters for DMA Profiling}

\sec Loading into Memory
\secc Direct loading of images

\chap Frameworks
\sec Kernel Support
\rfc{NNAPI is from Android not the Kernel NNAPI of the same name: \urlnote{https://developer.android.com/ndk/guides/neuralnetworks/}}
\sec LiteRT (Lite RunTime)

\rfc{formerly TFLite}

Models also need to be converted (and optimized)
into the FlatBuffers tflite format before use
\urlnote{https://ai.google.dev/edge/litert}.

We must set an environment variable which libvx checks
to see which device it should use, either NPU or GPU:

\begtt
export USE_GPU_INFERENCE=0
\endtt

Next we must load the external dynamic library
which we pass into the LiteRT interpreter:

\begtt\hisyntax{python}
import tflite_runtime.interpreter as tflite

# load external delegate
ext_delegate = [
    tflite.load_delegate("/usr/lib/libvx_delegate.so", "")
]
\endtt

\begtt\hisyntax{python}

interpreter = tflite.Interpreter(
model_path=args.model_file,
experimental_delegates=ext_delegate,
num_threads=args.num_threads)

runner(x=np.array(range(10), dtype=np.float32))
\endtt

\sec Tensor Virtual Machine (TVM)
\nocite[ExtendingTensoWang2022]
As the full name Apache TVM might suggest,
this is an open-source framework built by Apache,
to .

According to \code{EXTRA_OECMAKE} and the fact that it links against tim-vx
in the tvm bitbake recipe,
support for VSINPU should already be built in.
\rfc{Use as ONNX backend?}

\sec ONNX (Open Neural Network Exchange)

Though originally authored by joint efforts of Facebook and Microsoft,
\rfc{Source is wiki, find a better source: https://en.wikipedia.org/wiki/Open_Neural_Network_Exchange}
this project has flourished into a widely supported open-source ecosystem.
A comprehensive specification for models, formats, types, operators and
abstract data descriptions.
It includes protobuf definitions of their \code{.onnx} model files.
It used to support only inference,
but training was added with ONNX IR spec version 7\cite[OpenNeuralNet].
Models here are represented as either just a stateless
inference function in the case of inference-only models,
or may be extended with an initialization and a training
method which may modify the state of internal stateful variables
of the given model.
ONNX also encodes block of operators that encode a more complex task,
these may be substituted for builtins by the runtime based off of their name.\rfc{Was changed in IRv9}
The internal structure is a list of acyclically dependent topologically sorted nodes,
each node providing name, metadata, i/o and the given operation performed.
ONNX however does provide HOF-like operators that are applied
to entire subgraphs,
thus substituting the need for self-references\cite[OnnxConcepts].
These nodes are strung together as a pipeline each input being connected
to a previously declared output of the same name.
Each output name is unique since SSA is mandatory,
verification tools for this and other properties are
available from the creators of ONNX\rfc{SHOW}.
The last argument of some operators is marked as variadic,
thus allowing as is traditional with
regular languages to pass an arbitrary number of
inputs/outputs to it,
obviously respecting the minimum arity.
\rfc{ONNX vs. ONNX-ML}
These graphs are meant to be assembled programatically,
however ONNX does provide a textual form of their files.

ONNX requires all modules to be attributes of the class.


\secc NPU backend

Implementing a backend for ONNX is
done by providing a Python wrapping the functionality.

After downloading onnxruntime from pip,
I can use the available provider
CPU and Azure.
VSINPU is listed as known but unavailable.
Activating it yields an error rather than
a warning for Provider unsupported.

\secc Running on NPU

\begtt\hisyntax{python}
import onnxruntime
session = onnxruntime.InferenceSession("model.onnx", providers = [ "VSINPUExecutionProvider" ])
session.run(input_feed={"x": np.full(fill_value=[1.0], shape=(1,28,28), dtype=np.float32)}, output_names = ["add"])
outputs = session.run(None, {"input": inputTensor})
\endtt

NPU provider is added here\urlnote{https://github.com/microsoft/onnxruntime/blob/daf9565d1b550fd0d8149e8314bfce61d8df1d54/onnxruntime/core/providers/get_execution_providers.cc} line 89,
listed by \code{onnxruntime.get_available_providers},
yet then discarded when calling \urlnote{https://github.com/microsoft/onnxruntime/blob/daf9565d1b550fd0d8149e8314bfce61d8df1d54/onnxruntime/python/onnxruntime_pybind_state.cc} line 1238,
in favour of the \code{EP Unknown Provider falling back to CPU} error.
\rfc{I'd swear I had this working.}

\secc Tooling

\rfc{netron, zetane, tensor2onnx and such}

\secc Converting torch models into ONNX
\rfc{What exactly is "model" here?}
\begtt\hisyntax{python}
torch.onnx.export(
model,                                # model being run
torch.randn(1, 28, 28),    # model input (or a tuple for multiple inputs)
"fashion_mnist_model.onnx",           # where to save the model (can be a file or file-like object)
input_names = ['input'],              # the model's input names
output_names = ['output'])            # the model's output names
\endtt

\sec OpenCV

\begtt
cv2.error: OpenCV(4.10.0) /usr/src/debug/opencv/4.10.0.imx/modules/dnn/src/onnx/onnx_importer.cpp:1057: error: (-2:Unspecified error) in function 'handleNode'
> Node [Conv@ai.onnx]:(onnx_node!node_Conv_0) parse error: OpenCV(4.10.0) /usr/src/debug/opencv/4.10.0.imx/modules/dnn/src/layers/layers_common.cpp:106: error: (-5:Bad argument) kernel_size (or kernel_h and kernel_w) not specified in function 'getKernelSize
\endtt

\begtt\hisyntax{python}
import cv2 as cv
m = cv.dnn.readNet('model.onnx')
m.setPreferable
m.setPreferableBackend(cv.dnn.DNN_BACKEND_TIMVX)
m.setPreferableTarget(cv.dnn.DNN_TARGET_NPU)
import numpy as np
m.setInput(np.full(fill_value=[1.0], shape=(1,28,28)))
m.forward(["output"])
\endtt

Yields:
\begtt
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
cv2.error: OpenCV(4.10.0) /usr/src/debug/opencv/4.10.0.imx/modules/dnn/src/net_impl.cpp:279: error: (-204:Requested object was not found) Layer with requested id=-1 not found in function 'getLayerData'
\endtt
Which is the error for wrong output name.

\code{m.forward([])} and \code{m.forward()}
seem to both yield the same array,
seems that if no outputs are specified,
then implicitly it takes the one, \rfc{but which}.

\begtt
>>> m.getInputDetails()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
cv2.error: OpenCV(4.10.0) /usr/src/debug/opencv/4.10.0.imx/modules/dnn/src/net_quantization.cpp:268: error: (-6:Unknown error code -6) Net isn't quantized in function 'getInputDetails'
\endtt

\sec The NXP eIQ environment

The NXP eIQ stack officially supports the following engines:
\begitems
* LiteRT (formerly known as TensorFlow Lite)
* ONNX Runtime
* PyTorch
* OpenCV
\enditems
Of these only TensorFlow Lite supports the included NPU through the use of the VX delegate,
so called as it uses the \OpenVX~ library.

\chap Building the NXP IMX layer

Repo\urlnote{https://bugs.launchpad.net/lxml/+bug/2045435}
Adding
\begtt
CFLAGS += '-fpermissive'
or more specifically
CFLAGS += "-Wno-error=incompatible-pointer-types
\endtt
to \code{conf/local.conf} fixes it.
Optionally directly to the bbfiles of the problematic libraries,
since editing the global \code{local.conf} seems to cause all
artifacts to go out of date as it affects the entire tree,
so using the specific files:
\begitems
* \code{python3-lxml_5.0.0.bb}
* \code{expect_5.45.4.bb}
\enditems

I think, that should only rebuild a subset.

Nope, use a layer.

bitbake -c menuconfig virtual/kernel


Nope, just use a Docker container,
for example \code{gmacario/build-yocto}.
\begtt
apt install lz4 zstd
\endtt
\rfc{Files created under \code{build/tmp/work/imx8mpevk-poky-linux/imx-image-full/1.0/deploy-imx-image-full-image-complete}}

Using for second build
\code{docker run --rm -it -v /DISKB/cl-local/scarthgap.TQ.ARM.BSP.0001:/src --userns=keep-id 97bf1c8e3645}


\chap \OpenVX~

\VeriSilicon~ships their pre-compiled
shared library \code{libOpenVX.so} with TIM-VX.

\urlnote{https://www.nxp.com/docs/en/user-guide/IMX-MACHINE-LEARNING-UG.pdf}

\begitems
* APIs for Python and C++
* VX (i.MX 8 Series) / Ethos-U (i.MX 93) / Neutron (i.MX 95) Delegate
\enditems

\begitems \style d
* {VX Delegate} Connects TFLite to the \OpenVX~ API via TIM-VX
* {\OpenVX~ API} Callable API implemented by hardware vendors\cite[OpenvxPortab2011]
* {TIM-VX} Verisilicon Tensor Interface Module\urlnote{https://github.com/VeriSilicon/TIM-VX}
(used by the VX delegate)
\enditems

\sec Implementors
\begitems
* VeriSilicon/tflite-vx-delegate

Requires TIM-VX which is linked against the \VeriSilicon~\OpenVX~SDK\rfc{Is this proprietary or not}

* KhronosGroup/OpenVX-sample-impl

Only actually OSS implementation, however it is supposedly very ad-hoc and slowly implemented

* TexasInstruments/tiovx

Source available but only authorized for use on TI hardware
\enditems

\rfc{Relation between openvx and opencl}

\chap Speedup
\sec Theoretical advantages
\sec Benchmarking Practical Results

\sec Device Tensors
.\urlnote{https://onnxruntime.ai/docs/performance/device-tensor.html}

\sec Startup slowdowns

\secc Dynamic loading of Libraries

In the case of both the \code{Python} and \code{C++} interfaces,
using either \code{ONNX} or \code{LiteRT},
the case always is that the interfacing library
must be loaded dynamically.
Loading may take up a significant amount of time,
in one experiment after profiling with \code{perf}
shows 16.28\% of the runtime taken up by \code{do_lookup_x} from \code{ld-linux.so}.
\rfc{Be more specific about how long the total time took and what ran}

\secc Bus

Since the NPU acts as a completely separate device from the
standard SOC's CPU and memory,
even having its own clock,
we must transfer the model and input parameters
over a AXI/AHB\rfc{Specify what that is} bus
and that takes time.
Behind that Bus lies a Memory Controller,
scheduler and more which all work together to slow down the call
especially initially.

\chap Demonstrations

\chap Suggesting PikeOS integration

\app Glossary
\glos {NBG}{Network Binary Graph}
\glos {VsiNPU}{\VeriSilicon~NPU}
\glos {SSA}{Single Static Assignment}
\glos {Tensor}{Sufficiently described for purposes of this text as multidimensional arrays}
\glos {IR}{Intermediate Representation}

\makeglos

\bibchap
\usebib/c (iso690) Thesis

\app Attempts and scratchpad

\sec sudo bmaptool copy ~/cl-local/images/imx8mpevk/imx-image-full-imx8mpevk.rootfs.wic.zst /dev/mmcblk0

Did not connect at all

\sec sudo dd if=sdimagenxpi8mp.img.bkp of=/dev/mmcblk0 status=progress

Works, but doesn't boot `run mmcboot` provides a kernel error.

One of the three buttons is reset.
Left one, closest to V59 mark.

=> run mmcboot


bootd     - boot default, i.e., run 'bootcmd'
bootefi   - Boots an EFI payload from memory
bootelf   - Boot from an ELF image in memory
booti     - boot Linux kernel 'Image' format from memory
bootm     - boot application image from memory
bootp     - boot image via network using BOOTP/TFTP protocol
bootvx    - Boot vxWorks from an ELF image

Run sudo uuu -v -b sd uuu.auto-imx8mpevk
while switches in 0b0001 and pressing Reset.

sudo uuu -b emmc_all ./images/imx-boot-tqma8mpxl-mba8mpxl-mfgtool.bin-flash_spl_uboot ./images/tq-image-weston-debug-tqma8mpxl-mba8mpxl.rootfs.wic

Overlay
Must update some layers from BSP to NXP-IMX manifest version.
meta-freescale due to...
poky due to gstreamer version mismatch, meta-imx requires >=1.24.0,
but the bsp provided 1.22.5..

Don't forget to actually start booting i.e. select memory size, etc.

\rfc{The NPU can send CPU interrupts, these are set using the driver and cannot in fact be understood without the driver since the NPU and driver both assign arbitrary meaning to the given bits of the interrupt info register.}

Root of all property \code{./meta-freescale/recipes-graphics/imx-gpu-viv/imx-gpu-viv-6.inc}

\sec OPENCV Compilation

Compiling with external TIM-VX lib,
which is strongly advised against,
yields a segfaulting library.

\begtt\hisyntax{python}
>>> cv.setUseOpenVX(True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
cv2.error: OpenCV(4.10.0) /usr/src/debug/opencv/4.10.0.imx/modules/core/src/ovx.cpp:101: error: (-215:Assertion failed) !flag && "OpenVX support isn't enabled at compile time" in function 'setUseOpenVX'

>>> cv.useOpenVX()
False

self._model = cv.FaceDetectorYN.create(
            model=self._modelPath,
            config="",
            input_size=self._inputSize,
            score_threshold=self._confThreshold,
            nms_threshold=self._nmsThreshold,
            top_k=self._topK,
            backend_id=self._backendId,
            target_id=self._targetId)
	    # Segfaulted
\endtt
.\urlnote{https://github.com/opencv/opencv/wiki/TIM-VX-Backend-For-Running-OpenCV-On-NPU}

Now trying to compile TIM-VX directly in OpenCV,

\begtt
libgfortran was skipped: libgfortran needs fortran support to be enabled in the compiler
external-arm-toolchain PROVIDES libgfortran but was skipped: External ARM toolchain not configured (EAT_VER_MAIN not set).
\endtt

Apparently I need an external ARM toolchain such as Linaro.

\rfc{Does the TIM-VX driver even actually use the NPU?
It is universal after all.
With ONNX it is VSINPU so we can be quite sure,
LiteRT as well cause that is official,
OpenCV sets the target as NPU.}


It is strange that the NPU either finishes absolutely immediately,
or never finishes at all.
Even with smaller ones afterwards, perhaps the pipeline is clogged?


LiteRT seems to freeze at about 20000x2000 or so.
Cannot operate until restart

\begtt
2025-03-30 23:14:39.310737248 [W:onnxruntime:Default, vsinpu_ep_graph.cc:83 SupportedOp] Fallback unsupported op (node_unit) aten_pixel_shuffle  to cpu.
2025-03-30 23:14:39.313027340 [W:onnxruntime:Default, vsinpu_ep_graph.cc:83 SupportedOp] Fallback unsupported op (node_unit) Shape  to cpu.
2025-03-30 23:14:39.313115214 [W:onnxruntime:Default, base_op_builder.cc:138 HasSupportedInputOutputsImpl] Concat has unsupported input type : tensor(int64)
2025-03-30 23:14:39.313155463 [W:onnxruntime:Default, base_op_builder.cc:105 HasSupportedInputOutputs] Dynamic shape is not supported for now, for output:_inlfunc_aten_pixel_shuffle_reshaped_self
2025-03-30 23:14:39.313187213 [W:onnxruntime:Default, vsinpu_ep_graph.cc:83 SupportedOp] Fallback unsupported op (node_unit) DepthToSpace  to cpu.
2025-03-30 23:14:39.313213337 [W:onnxruntime:Default, vsinpu_ep_graph.cc:83 SupportedOp] Fallback unsupported op (node_unit) Shape  to cpu.
2025-03-30 23:14:39.313256337 [W:onnxruntime:Default, base_op_builder.cc:138 HasSupportedInputOutputsImpl] Concat has unsupported input type : tensor(int64)
2025-03-30 23:14:39.313292836 [W:onnxruntime:Default, base_op_builder.cc:56 operator()] Dynamic shape is not supported for now, for input:_inlfunc_aten_pixel_shuffle_depth_to_space
2025-03-30 23:14:39.324987167 [W:onnxruntime:Default, vsinpu_ep_graph.cc:83 SupportedOp] Fallback unsupported op (node_unit) DepthToSpace  to cpu.
2025-03-30 23:14:39.325322787 [W:onnxruntime:Default, vsinpu_execution_provider.cc:170 GetCapability] VSINPUExecutionProvider::GetCapability, number of partitions supported by VSINPU: 2; number of nodes in the graph: 10; number of nodes supported by VSINPU: 9
\endtt
.\rfc{Gather information on specifics of supported ops and sizes before proceeding}
.\rfc{In another instance RAM can overflow if the network is too large, however it must be exceedingly so.}
.\rfc{Visualizations\urlnote{https://netron.app/}}

\begtt
Samples: 20K of event 'cycles:P', Event count (approx.): 8246705352
Overhead  Command  Shared Object                                       Symbol
   7.98%  python3  ld-linux-aarch64.so.1                               [.] do_lookup_x                                                                                                         ◆
   6.47%  python3  libOpenVX.so.1.3.0                                  [.] fillinKernelBufferV8Huffman                                                                                         ▒
   5.58%  python3  libm.so.6                                           [.] __ceilf                                                                                                             ▒
   5.46%  python3  libOpenVX.so.1.3.0                                  [.] writeBits                                                                                                           ▒
   3.64%  python3  libOpenVX.so.1.3.0                                  [.] OutputAt                                                                                                            ▒
   3.48%  python3  libOpenVX.so.1.3.0                                  [.] calcKernelSizeV8Huffman                                                                                             ▒
   3.30%  python3  libc.so.6                                           [.] __memcpy_generic                                                                                                    ▒
   3.08%  python3  libtim-vx.so                                        [.] vsi_nn_Transpose                                                                                                    ▒
   2.28%  python3  libOpenVX.so.1.3.0                                  [.] calculateWeightNonZeroRatio                                                                                         ▒
   1.96%  python3  libNNArchPerf.so                                    [.] 0x00000000000024cc                                                                                                  ▒
   1.92%  python3  libNNArchPerf.so                                    [.] 0x00000000000024c4                                                                                                  ▒
   1.85%  python3  libpython3.12.so.1.0                                [.] _PyEval_EvalFrameDefault                                                                                            ▒
   1.51%  python3  libc.so.6                                           [.] __memset_zva64                                                                                                      ▒
   1.46%  python3  libNNArchPerf.so                                    [.] 0x00000000000024e0                                                                                                  ▒
   1.36%  python3  libOpenVX.so.1.3.0                                  [.] fillinTPKernelBufferHuffman                                                                                         ▒
   1.32%  python3  libOpenVX.so.1.3.0                                  [.] calcNonZeroCountV8Huffman                                                                                           ▒
   1.29%  python3  libOpenVX.so.1.3.0                                  [.] reorderKernelBufferV8HuffmanI8U8                                                                                    ▒
   1.02%  python3  libNNArchPerf.so                                    [.] ceilf@plt                                                                                                           ▒
   0.90%  python3  libNNArchPerf.so                                    [.] 0x00000000000024c0                                                                                                  ▒
   0.84%  python3  libOpenVX.so.1.3.0                                  [.] analysisKernelStreamForV8Huffman                                                                                    ▒
   0.73%  python3  ld-linux-aarch64.so.1                               [.] _dl_relocate_object                                                                                                 ▒
   0.65%  python3  ld-linux-aarch64.so.1                               [.] _dl_lookup_symbol_x                                                                                                 ▒
   0.63%  python3  libOpenVX.so.1.3.0                                  [.] analysisTPStreamForHuffman                                                                                          ▒
   0.59%  python3  [kernel.kallsyms]                                   [k] _raw_spin_unlock_irqrestore                                                                                         ▒
   0.52%  python3  libpython3.12.so.1.0                                [.] 0x00000000002c9078                                                                                                  ▒
   0.50%  python3  libNNArchPerf.so                                    [.] 0x00000000000024b8                                                                                                  ▒
   0.47%  python3  libNNArchPerf.so                                    [.] 0x00000000000024a8                                                                                                  ▒
   0.46%  python3  libNNArchPerf.so                                    [.] 0x00000000000024b0                                                                                                  ▒
   0.45%  python3  libNNArchPerf.so                                    [.] 0x00000000000024d4                                                                                                  ▒
   0.38%  python3  libNNArchPerf.so                                    [.] 0x00000000000024d8                                                                                                  ▒
   0.36%  python3  libpython3.12.so.1.0                                [.] 0x00000000002c88a8                                                                                                  ▒
   0.35%  python3  libOpenVX.so.1.3.0                                  [.] writeBits@plt                                                                                                       ▒
   0.33%  python3  libOpenVX.so.1.3.0                                  [.] minus8bitUint                                                                                                       ▒
   0.33%  python3  libOpenVX.so.1.3.0                                  [.] vxoContext_RemoveObject                                                                                             ▒
   0.29%  python3  libc.so.6                                           [.] _int_malloc                                                                                                         ▒
   0.29%  python3  libpython3.12.so.1.0                                [.] _PyType_Lookup                                                                                                      ▒
   0.27%  python3  libOpenVX.so.1.3.0                                  [.] reorderTPKernelBufferHuffman                                                                                        ▒
   0.26%  python3  ld-linux-aarch64.so.1                               [.] check_match                                                                                                         ▒
   0.25%  python3  libpython3.12.so.1.0                                [.] 0x00000000002c93f8                                                                                                  ▒
   0.25%  python3  ld-linux-aarch64.so.1                               [.] strcmp                                                                                                              ▒
   0.25%  python3  libNNArchPerf.so                                    [.] 0x0000000000001e00                                                                                                  ▒
   0.24%  python3  [kernel.kallsyms]                                   [k] __pi_clear_page                                                                                                     ▒
   0.24%  python3  libc.so.6                                           [.] malloc                                                                                                              ▒
   0.24%  python3  libc.so.6                                           [.] _int_free                                                                                                           ▒
   0.23%  python3  libOpenVX.so.1.3.0                                  [.] vxoContext_GetUserStructIndex                                                                                       ▒
   0.21%  python3  ld-linux-aarch64.so.1                               [.] _dl_tlsdesc_return                                                                                                  ▒
   0.20%  python3  libc.so.6                                           [.] malloc_consolidate                                                                                                  ▒
   0.20%  python3  libNNArchPerf.so                                    [.] APMCalcNNCycleCountBandWidth                                                                                        ▒
   0.16%  python3  [kernel.kallsyms]                                   [k] zap_pte_range                                                                                                       ▒
   0.15%  python3  libc.so.6                                           [.] cfree@GLIBC_2.17
\endtt

Python with and without Delegate:
\begtt
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Warm-up time: 137.6 ms

Inference time: 132.6 ms

0.870588: military uniform
0.031373: Windsor tie
0.011765: mortarboard
0.007843: bow tie
0.007843: bulletproof vest

real    0m1.436s
user    0m1.290s
sys     0m0.126s


Loading external delegate from /usr/lib/libvx_delegate.so with args: {}
INFO: Vx delegate: allowed_cache_mode set to 0.
INFO: Vx delegate: device num set to 0.
INFO: Vx delegate: allowed_builtin_code set to 0.
INFO: Vx delegate: error_during_init set to 0.
INFO: Vx delegate: error_during_prepare set to 0.
INFO: Vx delegate: error_during_invoke set to 0.
W [HandleLayoutInfer:332]Op 162: default layout inference pass.
Warm-up time: 3373.6 ms

Inference time: 3.2 ms

0.870588: military uniform
0.031373: Windsor tie
0.011765: mortarboard
0.007843: bow tie
0.007843: bulletproof vest

real    0m4.575s
user    0m4.254s
sys     0m0.263s
\endtt

C++ with and without Delegate:
\begtt
INFO: Loaded model mobilenet_v1_1.0_224_quant.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: invoked
INFO: average time: 34.498 ms
INFO: 0.768627: 653 military uniform
INFO: 0.105882: 907 Windsor tie
INFO: 0.0196078: 458 bow tie
INFO: 0.0117647: 466 bulletproof vest
INFO: 0.00784314: 835 suit

real    0m0.221s
user    0m0.511s
sys     0m0.024s


INFO: Loaded model mobilenet_v1_1.0_224_quant.tflite
INFO: resolved reporter
INFO: Vx delegate: allowed_cache_mode set to 0.
INFO: Vx delegate: device num set to 0.
INFO: Vx delegate: allowed_builtin_code set to 0.
INFO: Vx delegate: error_during_init set to 0.
INFO: Vx delegate: error_during_prepare set to 0.
INFO: Vx delegate: error_during_invoke set to 0.
INFO: EXTERNAL delegate created.
INFO: Applied EXTERNAL delegate.
W [HandleLayoutInfer:332]Op 162: default layout inference pass.
INFO: invoked
INFO: average time: 3.155 ms
INFO: 0.768627: 653 military uniform
INFO: 0.105882: 907 Windsor tie
INFO: 0.0196078: 458 bow tie
INFO: 0.0117647: 466 bulletproof vest
INFO: 0.00784314: 835 suit

real    0m3.460s
user    0m3.267s
sys     0m0.179s
\endtt

\bye
