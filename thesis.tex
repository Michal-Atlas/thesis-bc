\load[ctustyle3]
\worktype[B/EN]
\faculty{F8}
\department{Katedra Počítačových Systémů}
\title{NPU}
\author{Michal Žáček}
\abstractEN{D}
\abstractCZ{D}
\declaration{D}
\date{2024}
\draft

\def\OpenVX{OpenVX™}

\makefront

\chap Introduction

\sec Goals

This thesis will initially lay out the defined and known terms used in the context of neural nets, such as hardware, alternate names of known concepts or file formats.
Following this we shall go through the existing solutions implementing both the communication with actually hardware in the form of modules followed by frontend APIs for NPU work.
We will then go through the advantages that this hardware should in theory provide followed by a practical test of the actual results on real hardware.
These will be packaged into demonstration packages than may be run to show off and/or verify these results.
Finally, we discuss what this means for PikeOS integration of NPU support.

\sec NPUs
\sec Delegates
\sec File Formats

\secc Safetensors\urlnote{https://github.com/huggingface/safetensors}

\table{|l|l|}{ \crl
8 bytes & size of header \crl
N bytes & header \crl
rest & byte-buffer (little-endian) \crl
}

Header: JSON UTF-8 string, each top-level key is a tensor name the value of which contains a dictionary specifying the dtype, shape and data_offsets.

A schema\urlnote{https://github.com/huggingface/safetensors/blob/main/docs/safetensors.schema.json} is available.
It allows partial loading

\secc Pickle

{\bf\Red This is dangerous to use.}
Since pickling allows arbitrary instances of classes,
malicious code may be inserted as well into any model downloaded.

An example taken from \cite[hamann_exploiting_2020].
\verbinput \hisyntax{python} (-) pickle_exploit.py

\secc TFLite


\chap Analyses
\sec Backend
\secc The State of Kernel Support

\secc Manufacturer Modules

\sec Frontend
\secc Frameworks

\chap NXP eIQ environment
\sec Engines\urlnote{https://www.nxp.com/docs/en/user-guide/IMX-MACHINE-LEARNING-UG.pdf}

The NXP eIQ stack officially supports the following engines:
\begitems
* LiteRT (formerly known as TensorFlow Lite)
* ONNX Runtime
* PyTorch
* OpenCV
\enditems
Of these only TensorFlow Lite supports the included NPU through the use of the VX delegate,
so called as it uses the \OpenVX~ library.

\begitems
* APIs for Python and C++
* VX (i.MX 8 Series) / Ethos-U (i.MX 93) / Neutron (i.MX 95) Delegate
\enditems

\begitems \style d
* {Delegate} Kernel Module
* {VX Delegate} uses the \OpenVX~ library
* {\OpenVX} Callable API implemented by hardware vendors\cite[noauthor_openvx_2011]
* {\OpenVX~ library} software interface to \OpenVX
\enditems

\rfc{Relation between openvx and opencl}

\secc Language Bindings

\chap Speedup
\sec Theoretical advantages
\sec Benchmarking Practical Results
\sec Model conversion
eIQ toolkit
In the converted model,
the neutronGraph node is already generated. The neutron-delegate only captures the neutronGraph node and
offloads the work to Neutron-S. Inline compilation is not supported yet. [page 6]
\sec Startup slowdowns
\sec Quantization
\rfc{a.k.a. using narrower floats than the model was built with}
Since the NPU itself is built for 8/16-bit wide floats \cite[lf_imx_2024].

\chap Demonstrations
\chap Suggesting PikeOS integration

\chap YOCTO layers
\urlnote{https://docs.yoctoproject.org/dev/dev-manual/layers.html}
% https://www.nxp.com/docs/en/user-guide/IMX_YOCTO_PROJECT_USERS_GUIDE.pdf
\chap OpenEmbedded build system

\bibchap
\usebib/c (iso690) Thesis

\bye
