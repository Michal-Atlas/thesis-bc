\load[ctustyle3]
\worktype[B/EN]
\faculty{F8}
\department{Katedra Počítačových Systémů}
\title{NPU}
\author{Michal Žáček}
\abstractEN{D}
\abstractCZ{D}
\declaration{D}
\date{2024}
\draft

\def\OpenVX{OpenVX™}

\makefront

\chap Introduction

Our computing devices have come a long way in terms of speed
and of course in terms of operating complexity.
This complexity can hold back very general purpose processing units
in performance when it comes to highly specialized tasks.
For a long time GPUs were used where a large number
of specialized identical operations needed to be performed on
a vast array of differing data inputs,
as their specialized design for this use-case made them incomparably faster than
using a common general purpose CPU.
Though most often associated with their namesake of graphics,
they excel at many applications and have become a
valuable tool when working with neural networks.

Recently even more specialized hardware has become available
to meet the rising demand in having access to these technologies
in more portable devices or for use in embedded situations.
Neural Processing Units\fnote{Historically sometimes called VPUs, V standing for Versatile},
are becoming more common,
however these don't often come with universal drivers
and the most widespread standard today has no freely adoptable implementation.

\sec Goals

This thesis will initially lay out the defined and known terms used in the context of neural nets, such as hardware, alternate names of known concepts or file formats.
Following this we shall go through the existing solutions implementing both the communication with actually hardware in the form of modules followed by frontend APIs for NPU work.
We will then go through the advantages that this hardware should in theory provide followed by a practical test of the actual results on real hardware.
These will be packaged into demonstration packages than may be run to show off and/or verify these results.
Finally, we discuss what this means for PikeOS integration of NPU support.

\sec NPUs
\sec Delegates
\sec File Formats

\secc Safetensors

\midinsert \clabel[safetensordef]{SafeTensor format structure}
\ctable{ll}{
Size & Content \crl
8 bytes & size of header \cr
N bytes & header \cr
rest & byte-buffer (little-endian)
}
\caption/t SafeTensor format structure
\endinsert

Header\urlnote{https://github.com/huggingface/safetensors}:
JSON UTF-8 string, each top-level key is a tensor name the value of which contains a dictionary specifying the dtype, shape and data_offsets.

A schema\urlnote{https://github.com/huggingface/safetensors/blob/main/docs/safetensors.schema.json} is available.
It allows partial loading

\secc Pickle

{\bf\Red This is dangerous to use.}
Since pickling allows arbitrary instances of classes,
malicious code may be inserted as well into any model downloaded.

An example taken from \cite[hamann_exploiting_2020].
\verbinput \hisyntax{python} (-) pickle_exploit.py

\secc LiteRT (Lite RunTime)

\rfc{formerly TFLite}

\seccc Relation to Tensorflow

Tensorflow is used to train models and facilitate actual ML work.
LiteRT is stripped down to a lightweight runtime
for embedded use and can only run models.
Models also need to be converted (and optimized)
into the FlatBuffers tflite format before use
\urlnote{https://ai.google.dev/edge/litert}.

\secc \code{.bmap}

Relates to Yocto project's \code{bmaptool}\urlnote{https://github.com/yoctoproject/bmaptool}.
\rfc{It's not yocto but forked from intel?}

\secc \code{.spdx}

\secc \code{.wic}

\secc \code{.wks}


\chap Analyses
\sec Backend
\secc The State of Kernel Support
\rfc{NNAPI is from Android not the Kernel NNAPI of the same name: \ulink[link]{https://developer.android.com/ndk/guides/neuralnetworks/}}
\secc Manufacturer Modules

\sec Frontend
\secc Frameworks

\chap NXP IMX

Repo\urlnote{https://bugs.launchpad.net/lxml/+bug/2045435}
Adding
\begtt
CFLAGS += '-fpermissive'
or more specifically
CFLAGS += "-Wno-error=incompatible-pointer-types
\endtt
to \code{conf/local.conf} fixes it.
Optionally directly to the bbfiles of the problematic libraries,
since editing the global \code{local.conf} seems to cause all
artifacts to go out of date as it affects the entire tree,
so using the specific files:
\begitems
* \code{python3-lxml_5.0.0.bb}
* \code{expect_5.45.4.bb}
\enditems

I think, that should only rebuild a subset.

Nope, use a layer.

bitbake -c menuconfig virtual/kernel


Nope, just use a Docker container,
for example \code{gmacario/build-yocto}.


\chap NXP eIQ environment
\chap Engines

\sec \OpenVX~

\urlnote{https://www.nxp.com/docs/en/user-guide/IMX-MACHINE-LEARNING-UG.pdf}

The NXP eIQ stack officially supports the following engines:
\begitems
* LiteRT (formerly known as TensorFlow Lite)
* ONNX Runtime
* PyTorch
* OpenCV
\enditems
Of these only TensorFlow Lite supports the included NPU through the use of the VX delegate,
so called as it uses the \OpenVX~ library.

\begitems
* APIs for Python and C++
* VX (i.MX 8 Series) / Ethos-U (i.MX 93) / Neutron (i.MX 95) Delegate
\enditems

\begitems \style d
* {VX Delegate} Connects TFLite to the \OpenVX~ API via TIM-VX
* {\OpenVX~ API} Callable API implemented by hardware vendors\cite[noauthor_openvx_2011]
* {TIM-VX} Verisilicon Tensor Interface Module\urlnote{https://github.com/VeriSilicon/TIM-VX}
(used by the VX delegate)
\enditems

\secc Implementors
\begitems
* VeriSilicon/tflite-vx-delegate

  Requires TIM-VX which is linked against the VeriSilicon OpenVX SDK\rfc{Is this proprietary or not}

* KhronosGroup/OpenVX-sample-impl

  Only actually OSS implementation, however it is supposedly very ad-hoc and slowly implemented

* TexasInstruments/tiovx

  Source available but only authorized for use on TI hardware
\enditems

\rfc{Relation between openvx and opencl}

\sec Apache TVM
\sec ONNX Runtime

\secc Language Bindings

\chap Speedup
\sec Theoretical advantages
\sec Benchmarking Practical Results
\sec Model conversion
eIQ toolkit
In the converted model,
the neutronGraph node is already generated. The neutron-delegate only captures the neutronGraph node and
offloads the work to Neutron-S. Inline compilation is not supported yet. [page 6]
\sec Startup slowdowns
\sec Quantization
\rfc{a.k.a. using narrower floats than the model was built with}
Since the NPU itself is built for 8/16-bit wide floats \cite[lf_imx_2024].

\chap Demonstrations
\chap Suggesting PikeOS integration

\chap YOCTO layers
\urlnote{https://docs.yoctoproject.org/dev/dev-manual/layers.html}

\chap OpenEmbedded build system

\bibchap
\usebib/c (iso690) Thesis

\app Glossary
\glos {NBG}{Network Binary Graph}

\makeglos

\bye
